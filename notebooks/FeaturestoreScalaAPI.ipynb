{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>47</td><td>application_1545231463715_0067</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1545231463715_0067/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1545231463715_0067_01_000001/fs_demo__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import io.hops.util.Hops\n",
      "import scala.collection.JavaConversions._\n",
      "import collection.JavaConverters._\n"
     ]
    }
   ],
   "source": [
    "import io.hops.util.Hops\n",
    "import scala.collection.JavaConversions._\n",
    "import collection.JavaConverters._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Featurestore\n",
    "\n",
    "Each project with the featurestore enabled gets its own Hive database for the featurestore, the name of the featurestore database is 'projectname_featurestore' and can be retrieved from the hops-util-py featurestore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = fs_demo_featurestore\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Featurestores Accessible in the Current Project\n",
    "\n",
    "Feature stores can be shared across projects just like other Hopsworks datasets. You can use this API function to list all the featurestores accessible in the project programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: java.util.List[String] = [fs_demo_featurestore]\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Individual Feature\n",
    "\n",
    "When retrieving a single feature from the featurestore, the hops-util-py library will infer which featuregroup the feature belongs to by querying the metastore, but you can also explicitly specify which featuregroup and version to query. If there are multiple features of the same name in the featurestore, it is required to specify enough information to uniquely identify the feature (e.g which featuregroup and which version).  If no featurestore is provided it will default to the project's featurestore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying featuregroup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|action|\n",
      "+------+\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeature(spark, \"action\", Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With specifed featuregroup and version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|action|\n",
      "+------+\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeature(spark, \"action\", Hops.getProjectFeaturestore, \"web_logs_features\", 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Featuregroup\n",
    "\n",
    "You can get an entire featuregroup from the API. If no featurestore is provided the API will default to the project's featurestore, if no version is provided it will default to version 1 of the featuregroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+---------+---------+\n",
      "|  avg_trx|count_trx|cust_id|  max_trx|  min_trx|\n",
      "+---------+---------+-------+---------+---------+\n",
      "| 1090.509|       16|    148|2094.9958| 390.4109|\n",
      "| 738.1404|       16|    496|1464.5397| 9.235389|\n",
      "|899.89594|       30|    463|1828.2426|33.797318|\n",
      "|607.17773|        4|    471|636.18713|578.16833|\n",
      "| 698.5791|       28|    243| 1582.427|119.73669|\n",
      "+---------+---------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"trx_summary_features\", Hops.getProjectFeaturestore, 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Set of Features\n",
    "\n",
    "When retrieving a list of features from the featurestore, the hops-util-py library will infer which featuregroup the features belongs to by querying the metastore. If the features reside in different featuregroups, the library will also **try** to infer how to join the features together based on common columns. If the JOIN query cannot be inferred due to existence of multiple features with the same name or non-obvious JOIN query, the user need to supply enough information to the API call to be able to query the featurestore. If the user already knows the JOIN query it can also run `Hops.queryFeaturestore(joinQuery)` directly (an example of using this approach is shown further down in this notebook). If no featurestore is provided it will default to the project's featurestore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: List[String] = List(pagerank, triangle_count, avg_trx)\n"
     ]
    }
   ],
   "source": [
    "val features = List(\"pagerank\", \"triangle_count\", \"avg_trx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Found the feature with name: avg_trx in more than one of the featuregroups of the featurestore fs_demo_featurestore please specify featuregroup that you want to get the feature from. The matched featuregroups are: pep_lookup_1, customer_type_lookup_1, trx_type_lookup_1, gender_lookup_1, industry_sector_lookup_1, country_lookup_1, alert_type_lookup_1, rule_name_lookup_1, browser_action_lookup_1, web_address_lookup_1, demographic_features_1, alert_features_1, trx_graph_summary_features_1, trx_features_1, trx_summary_features_1, hipo_features_1, trx_graph_edge_list_1, police_report_features_1, web_logs_features_1, trx_summary_features_2_1\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeature(FeaturestoreHelper.java:298)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeaturegroupsThatContainsFeatures(FeaturestoreHelper.java:255)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1504)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the join key but specifying featuregroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroupsMap: scala.collection.immutable.Map[String,Integer] = Map(trx_graph_summary_features -> 1, trx_summary_features -> 1)\n",
      "javaFeaturegroupsMap: java.util.HashMap[String,Integer] = {trx_summary_features=1, trx_graph_summary_features=1}\n"
     ]
    }
   ],
   "source": [
    "val featuregroupsMap = Map[String, Integer](\"trx_graph_summary_features\"->1,\"trx_summary_features\"->1)\n",
    "val javaFeaturegroupsMap = new java.util.HashMap[String, Integer](featuregroupsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------+\n",
      "|pagerank|triangle_count|  avg_trx|\n",
      "+--------+--------------+---------+\n",
      "|     1.0|           3.0|963.64233|\n",
      "|     1.0|          12.0| 746.5783|\n",
      "|     1.0|           7.0|687.91376|\n",
      "|     1.0|          12.0| 732.6695|\n",
      "|     1.0|           4.0|  641.785|\n",
      "+--------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore, javaFeaturegroupsMap).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying both featuregroups and join key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------+\n",
      "|pagerank|triangle_count|  avg_trx|\n",
      "+--------+--------------+---------+\n",
      "|     1.0|           3.0|963.64233|\n",
      "|     1.0|          12.0| 746.5783|\n",
      "|     1.0|           7.0|687.91376|\n",
      "|     1.0|          12.0| 732.6695|\n",
      "|     1.0|           4.0|  641.785|\n",
      "+--------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore, javaFeaturegroupsMap, \"cust_id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting 10 features from two different featuregroups without specifying the featuregroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Found the feature with name: avg_trx in more than one of the featuregroups of the featurestore fs_demo_featurestore please specify featuregroup that you want to get the feature from. The matched featuregroups are: pep_lookup_1, customer_type_lookup_1, trx_type_lookup_1, gender_lookup_1, industry_sector_lookup_1, country_lookup_1, alert_type_lookup_1, rule_name_lookup_1, browser_action_lookup_1, web_address_lookup_1, demographic_features_1, alert_features_1, trx_graph_summary_features_1, trx_features_1, trx_summary_features_1, hipo_features_1, trx_graph_edge_list_1, police_report_features_1, web_logs_features_1, trx_summary_features_2_1\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeature(FeaturestoreHelper.java:298)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeaturegroupsThatContainsFeatures(FeaturestoreHelper.java:255)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1504)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features1 = List(\"pagerank\", \"triangle_count\", \"avg_trx\", \"count_trx\", \"max_trx\", \"min_trx\", \"balance\", \"birthdate\", \"join_date\", \"number_of_accounts\")\n",
    "Hops.getFeatures(spark, features1, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to get features that exist in multiple featuregroups, the library will not be able to infer from which featuregroup to get the features, so you must specify the featuregroups explicitly as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Found the feature with name: avg_trx in more than one of the featuregroups of the featurestore fs_demo_featurestore please specify featuregroup that you want to get the feature from. The matched featuregroups are: pep_lookup_1, customer_type_lookup_1, trx_type_lookup_1, gender_lookup_1, industry_sector_lookup_1, country_lookup_1, alert_type_lookup_1, rule_name_lookup_1, browser_action_lookup_1, web_address_lookup_1, demographic_features_1, alert_features_1, trx_graph_summary_features_1, trx_features_1, trx_summary_features_1, hipo_features_1, trx_graph_edge_list_1, police_report_features_1, web_logs_features_1, trx_summary_features_2_1\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeature(FeaturestoreHelper.java:298)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeaturegroupsThatContainsFeatures(FeaturestoreHelper.java:255)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1504)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features2 = List(\"pagerank\", \"triangle_count\", \"avg_trx\", \"count_trx\", \"max_trx\", \"min_trx\", \"balance\", \"birthdate\", \"join_date\", \"number_of_accounts\", \"pep\")\n",
    "Hops.getFeatures(spark, features2, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we specify the featuregroup to get the feature that exists in multiple featuregroups, the library can infer how to get the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroupsMap1: scala.collection.immutable.Map[String,Integer] = Map(trx_graph_summary_features -> 1, trx_summary_features -> 1, demographic_features -> 1)\n",
      "javaFeaturegroupsMap1: java.util.HashMap[String,Integer] = {demographic_features=1, trx_summary_features=1, trx_graph_summary_features=1}\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+-------------+\n",
      "|pagerank|triangle_count|  avg_trx|count_trx|  max_trx|  min_trx|  balance|          birthdate|          join_date|number_of_accounts|          pep|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+-------------+\n",
      "|     1.0|           5.0| 1090.509|       16|2094.9958| 390.4109|12920.496|2003-04-12 00:00:00|1998-09-06 00:00:00|                10| 309237645312|\n",
      "|     1.0|           5.0| 738.1404|       16|1464.5397| 9.235389| 11096.28|1985-09-14 00:00:00|2016-07-06 00:00:00|                 7|1331439861760|\n",
      "|     1.0|           6.0|899.89594|       30|1828.2426|33.797318|1868.0168|2006-09-07 00:00:00|1973-02-13 00:00:00|                14| 309237645312|\n",
      "|     1.0|           4.0|607.17773|        4|636.18713|578.16833| 9278.589|2018-07-10 00:00:00|1999-02-25 00:00:00|                 1| 309237645312|\n",
      "|     1.0|           9.0| 698.5791|       28| 1582.427|119.73669| 593.9806|1979-04-12 00:00:00|2017-12-07 00:00:00|                11|1331439861760|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val featuregroupsMap1 = Map[String, Integer](\n",
    "    \"trx_graph_summary_features\"->1,\n",
    "    \"trx_summary_features\"->1,\n",
    "    \"demographic_features\" ->1\n",
    ")\n",
    "val javaFeaturegroupsMap1 = new java.util.HashMap[String, Integer](featuregroupsMap1)\n",
    "Hops.getFeatures(spark, features2, Hops.getProjectFeaturestore, javaFeaturegroupsMap1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of getting 19 features from 5 different featuregroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features3: List[String] = List(pagerank, triangle_count, avg_trx, count_trx, max_trx, min_trx, balance, birthdate, join_date, number_of_accounts, pep, customer_type, gender, web_id, time_spent_seconds, address, action, report_date, report_id)\n",
      "featuregroupsMap2: scala.collection.immutable.Map[String,Integer] = Map(police_report_features -> 1, web_logs_features -> 1, trx_graph_summary_features -> 1, trx_summary_features -> 1, demographic_features -> 1)\n",
      "javaFeaturegroupsMap2: java.util.HashMap[String,Integer] = {demographic_features=1, police_report_features=1, web_logs_features=1, trx_summary_features=1, trx_graph_summary_features=1}\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+\n",
      "|pagerank|triangle_count|  avg_trx|count_trx|  max_trx|  min_trx|  balance|          birthdate|          join_date|number_of_accounts|         pep|customer_type|      gender|web_id|time_spent_seconds|address|action|        report_date|report_id|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  4756|               300|      0|     0|2011-03-26 00:00:00|        1|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  3517|               762|      1|     0|2011-03-26 00:00:00|        1|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1996|               271|      0|     0|2011-03-26 00:00:00|        1|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1342|               197|      0|     0|2011-03-26 00:00:00|        1|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1278|               102|      1|     0|2011-03-26 00:00:00|        1|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features3 = List(\"pagerank\", \"triangle_count\", \"avg_trx\", \"count_trx\", \"max_trx\", \"min_trx\",\n",
    "    \"balance\", \"birthdate\", \"join_date\", \"number_of_accounts\", \"pep\", \"customer_type\", \"gender\", \"web_id\",\n",
    "    \"time_spent_seconds\", \"address\", \"action\", \"report_date\", \"report_id\")\n",
    "val featuregroupsMap2 = Map[String, Integer](\n",
    "    \"trx_graph_summary_features\"->1,\n",
    "    \"trx_summary_features\"->1,\n",
    "    \"demographic_features\" ->1,\n",
    "    \"web_logs_features\" -> 1,\n",
    "    \"police_report_features\" -> 1\n",
    ")\n",
    "val javaFeaturegroupsMap2 = new java.util.HashMap[String, Integer](featuregroupsMap2)\n",
    "Hops.getFeatures(spark, features3, Hops.getProjectFeaturestore, javaFeaturegroupsMap2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might want to get a feature that exist in multiple featuregroups and you want to include all of these featuregroups in your query, then you can specify from which of the featuregroup to get the feature by prepending the feature-name with the featuregroup name + '_version', e.g: 'demographic_features_1.cust_id'. If you don't specify this the query will fail as the library won't know from which of your specified featuregroups to get the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.AnalysisException: Reference 'cust_id' is ambiguous, could be: demographic_features_1.cust_id, police_report_features_1.cust_id, web_logs_features_1.cust_id, trx_summary_features_1.cust_id, trx_graph_summary_features_1.cust_id.; line 1 pos 219\n",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:213)\n",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:97)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$37.apply(Analyzer.scala:826)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$37.apply(Analyzer.scala:828)\n",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:825)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "  at scala.collection.immutable.List.map(List.scala:296)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:895)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:837)\n",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:293)\n",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:293)\n",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:837)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)\n",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n",
      "  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n",
      "  at scala.collection.immutable.List.foldLeft(List.scala:84)\n",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.logAndRunSQL(FeaturestoreHelper.java:563)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.getFeatures(FeaturestoreHelper.java:496)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1467)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features4 = List(\"pagerank\", \"triangle_count\", \"avg_trx\", \"count_trx\", \"max_trx\", \"min_trx\",\n",
    "    \"balance\", \"birthdate\", \"join_date\", \"number_of_accounts\", \"pep\", \"customer_type\", \"gender\", \"web_id\",\n",
    "    \"time_spent_seconds\", \"address\", \"action\", \"report_date\", \"report_id\", \"cust_id\")\n",
    "Hops.getFeatures(spark, features4, Hops.getProjectFeaturestore, javaFeaturegroupsMap2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change 'cust_id' to 'featuregroupname_version.cust_id' the library knows where to get the feature from and the query works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features5: List[String] = List(pagerank, triangle_count, avg_trx, count_trx, max_trx, min_trx, balance, birthdate, join_date, number_of_accounts, pep, customer_type, gender, web_id, time_spent_seconds, address, action, report_date, report_id, demographic_features_1.cust_id)\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+-------+\n",
      "|pagerank|triangle_count|  avg_trx|count_trx|  max_trx|  min_trx|  balance|          birthdate|          join_date|number_of_accounts|         pep|customer_type|      gender|web_id|time_spent_seconds|address|action|        report_date|report_id|cust_id|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+-------+\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  4756|               300|      0|     0|2011-03-26 00:00:00|        1|      9|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  3517|               762|      1|     0|2011-03-26 00:00:00|        1|      9|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1996|               271|      0|     0|2011-03-26 00:00:00|        1|      9|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1342|               197|      0|     0|2011-03-26 00:00:00|        1|      9|\n",
      "|     1.0|           1.0|599.04565|       18|1283.6562|36.825226|15603.314|1995-05-13 00:00:00|2015-01-01 00:00:00|                14|309237645312| 420906795008|566935683072|  1278|               102|      1|     0|2011-03-26 00:00:00|        1|      9|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+-------------------+-------------------+------------------+------------+-------------+------------+------+------------------+-------+------+-------------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features5 = List(\"pagerank\", \"triangle_count\", \"avg_trx\", \"count_trx\", \"max_trx\", \"min_trx\",\n",
    "    \"balance\", \"birthdate\", \"join_date\", \"number_of_accounts\", \"pep\", \"customer_type\", \"gender\", \"web_id\",\n",
    "    \"time_spent_seconds\", \"address\", \"action\", \"report_date\", \"report_id\", \"demographic_features_1.cust_id\")\n",
    "Hops.getFeatures(spark, features5, Hops.getProjectFeaturestore, javaFeaturegroupsMap2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Text Query from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex queries that cannot be inferred by the helper functions, enter the sql directly to the method `Hops.queryFeaturestore()` it will default to the project specific feature store but you can also specify it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the featurestore it will default to the project-specific featurestore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|cust_id|pagerank|triangle_count|\n",
      "+-------+--------+--------------+\n",
      "|     29|     1.0|          12.0|\n",
      "|    474|     1.0|           7.0|\n",
      "|     65|     1.0|          12.0|\n",
      "|    222|     1.0|          13.0|\n",
      "|    270|     1.0|           8.0|\n",
      "+-------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.queryFeaturestore(\n",
    "    spark,\n",
    "    \"SELECT * FROM trx_graph_summary_features_1 WHERE triangle_count > 5\",\n",
    "    null\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the featurestore to query explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|cust_id|pagerank|triangle_count|\n",
      "+-------+--------+--------------+\n",
      "|     29|     1.0|          12.0|\n",
      "|    474|     1.0|           7.0|\n",
      "|     65|     1.0|          12.0|\n",
      "|    222|     1.0|          13.0|\n",
      "|    270|     1.0|           8.0|\n",
      "+-------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.queryFeaturestore(\n",
    "    spark,\n",
    "    \"SELECT * FROM trx_graph_summary_features_1 WHERE triangle_count > 5\",\n",
    "    Hops.getProjectFeaturestore\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to the Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get some sample data to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleDataMap: scala.collection.immutable.Map[String,Int] = Map(hops_customer_1 -> 3, hops_customer_2 -> 4)\n",
      "sampleDataDf: org.apache.spark.sql.DataFrame = [customer_type: string, id: int]\n"
     ]
    }
   ],
   "source": [
    "val sampleDataMap = Map(\"hops_customer_1\"-> 3, \"hops_customer_2\"-> 4)\n",
    "val sampleDataDf = sampleDataMap.toSeq.toDF(\"customer_type\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|  customer_type| id|\n",
      "+---------------+---+\n",
      "|hops_customer_1|  3|\n",
      "|hops_customer_2|  4|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDataDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the contents of the featuregroup 'customer_type_lookup' that we are going to insert the sample data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [customer_type: string, id: bigint]\n"
     ]
    }
   ],
   "source": [
    "val sparkDf = Hops.getFeaturegroup(spark, \"customer_type_lookup\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|  customer_type| id|\n",
      "+---------------+---+\n",
      "|hops_customer_1|  3|\n",
      "|hops_customer_2|  4|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: Long = 2\n"
     ]
    }
   ],
   "source": [
    "sparkDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can insert the sample data and verify the new contents of the featuregroup. By default the insert mode is \"append\", the featurestore is the project's featurestore and the version is 1 (the statistics part will be covered later in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = customer_type_lookup\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "mode: String = append\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = trx_summary_features without the column count_trx\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"customer_type_lookup\"\n",
    "val featurestore = Hops.getProjectFeaturestore \n",
    "val featuregroupVersion = 1 \n",
    "val mode = \"append\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"trx_summary_features without the column count_trx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.insertIntoFeaturegroup(\n",
    "    sampleDataDf, \n",
    "    spark, \n",
    "    featuregroup,\n",
    "    featurestore,\n",
    "    featuregroupVersion,\n",
    "    mode,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|  customer_type| id|\n",
      "+---------------+---+\n",
      "|hops_customer_1|  3|\n",
      "|hops_customer_1|  3|\n",
      "|hops_customer_2|  4|\n",
      "|hops_customer_2|  4|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"customer_type_lookup\", Hops.getProjectFeaturestore, 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res22: Long = 4\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"customer_type_lookup\", Hops.getProjectFeaturestore, 1).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two supported insert modes are \"append\" and \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: String = overwrite\n"
     ]
    }
   ],
   "source": [
    "val mode = \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.insertIntoFeaturegroup(\n",
    "    sampleDataDf, \n",
    "    spark, \n",
    "    featuregroup,\n",
    "    featurestore,\n",
    "    featuregroupVersion,\n",
    "    mode,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|  customer_type| id|\n",
      "+---------------+---+\n",
      "|hops_customer_1|  3|\n",
      "|hops_customer_2|  4|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"customer_type_lookup\", Hops.getProjectFeaturestore, 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res25: Long = 2\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"customer_type_lookup\", Hops.getProjectFeaturestore, 1).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Featuregroup From a Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases it is recommended that featuregroups are created in the UI on Hopsworks and that care is taken in documenting the featuregroup. However, sometimes it is practical to create a featuregroup directly from a spark dataframe and fill in the metadata about the featuregroup later in the UI. This can be done through the create_featuregroup API function.\n",
    "\n",
    "Lets create a new featuregroup that contains the same contents as the featuregroup trx_summary except the the column count_trx is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trxSummaryDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [avg_trx: float, count_trx: bigint ... 3 more fields]\n",
      "trxSummaryDf1: org.apache.spark.sql.DataFrame = [avg_trx: float, cust_id: int ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val trxSummaryDf = Hops.getFeaturegroup(spark, \"trx_summary_features\", Hops.getProjectFeaturestore, 1)\n",
    "val trxSummaryDf1 = trxSummaryDf.drop(\"count_trx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+\n",
      "|  avg_trx|cust_id|  max_trx|  min_trx|\n",
      "+---------+-------+---------+---------+\n",
      "| 1090.509|    148|2094.9958| 390.4109|\n",
      "| 738.1404|    496|1464.5397| 9.235389|\n",
      "|899.89594|    463|1828.2426|33.797318|\n",
      "|607.17773|    471|636.18713|578.16833|\n",
      "| 698.5791|    243| 1582.427|119.73669|\n",
      "+---------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trxSummaryDf1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a feature group is created you can specify metadata about the feature group or set it to null and fill it in later in the feature registry UI. (The statistics part will be explained later on in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = trx_summary_features without the column count_trx\n"
     ]
    }
   ],
   "source": [
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"trx_summary_features without the column count_trx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a new featuregroup using the transformed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.createFeaturegroup(\n",
    "    spark, trxSummaryDf1, \"trx_summary_features_2\", Hops.getProjectFeaturestore,\n",
    "    1, description, jobId,\n",
    "    dependencies, primaryKey, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Featuregroup Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics about a featuregroup can be useful in the stage of feature engineering and when deciding which features to use for training.\n",
    "\n",
    "To compute statistics about an existing featuregroup (that should not be empty of course), you can use the API call update_featuregroup_stats. By default it will compute all statistics (descriptive, feature correlation, histograms, and cluster analysis), use the project's featurestore, use version 1 of the featuregroup and use all columns for computing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = trx_summary_features\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "descriptiveStats: Boolean = true\n",
      "featureCorr: Boolean = true\n",
      "featureHistograms: Boolean = true\n",
      "clusterAnalysis: Boolean = true\n",
      "statColumns: Null = null\n",
      "numBins: Int = 20\n",
      "corrMethod: String = pearson\n",
      "numClusters: Int = 5\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"trx_summary_features\"\n",
    "val featurestore = Hops.getProjectFeaturestore\n",
    "val featuregroupVersion = 1\n",
    "val descriptiveStats = true\n",
    "val featureCorr = true\n",
    "val featureHistograms = true\n",
    "val clusterAnalysis = true\n",
    "val statColumns = null // null means all columns will be used\n",
    "val numBins = 20\n",
    "val corrMethod = \"pearson\"\n",
    "val numClusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.updateFeaturegroupStats(\n",
    "    spark, featuregroup, Hops.getProjectFeaturestore, featuregroupVersion,\n",
    "    descriptiveStats, featureCorr, featureHistograms, clusterAnalysis, statColumns,\n",
    "    numBins, corrMethod, numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Managed Training Datasets From Sets of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have found the features you need in the featurestore you can materialize the features into a training dataset so that you can train a machine learning model using the features. Just as for featuregroups, it is useful to version and document training datasets, for this reason HopsML supports **managed training datasets** which enables you to easily version, document and automate the materialization of training datasets.\n",
    "\n",
    "Metadata for a training dataset can be created from the Hopsworks UI or directly from the API with the function create_training_dataset. The training datasets in a project are stored in a top-level dataset called Training_Datasets, (i.e `hdfs:///Projects/<ProjectName>/Training_Datasets`.\n",
    "\n",
    "Once a training dataset have been created you can find it in the featurestore UI in hopsworks under the tab Training datasets, from there you can also edit the metadata if necessary. After a training dataset have been created with the necessary metadata you can save the actual data in the training dataset by using the API function insert_into_training_dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataset called `AML_dataset` by using a set of relevant features from the featurestore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select the features (and/or labels) that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: List[String] = List(pagerank, triangle_count, avg_trx, count_trx, max_trx, min_trx, balance, number_of_accounts, pep)\n",
      "featuregroupsToVersionMap: scala.collection.immutable.Map[String,Integer] = Map(trx_graph_summary_features -> 1, trx_summary_features -> 1, demographic_features -> 1)\n",
      "javaFeaturegroupsMap: java.util.HashMap[String,Integer] = {demographic_features=1, trx_summary_features=1, trx_graph_summary_features=1}\n"
     ]
    }
   ],
   "source": [
    "val features = List(\"pagerank\", \n",
    "                    \"triangle_count\", \n",
    "                    \"avg_trx\", \n",
    "                    \"count_trx\", \n",
    "                    \"max_trx\", \n",
    "                    \"min_trx\", \n",
    "                    \"balance\", \n",
    "                    \"number_of_accounts\", \n",
    "                    \"pep\")\n",
    "val featuregroupsToVersionMap = Map[String, Integer](\n",
    "    \"trx_graph_summary_features\"->1,\n",
    "    \"trx_summary_features\"->1,\n",
    "    \"demographic_features\" ->1\n",
    ")\n",
    "val javaFeaturegroupsMap = new java.util.HashMap[String, Integer](featuregroupsToVersionMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [pagerank: float, triangle_count: float ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "val datasetDf = Hops.getFeatures(spark, features, Hops.getProjectFeaturestore, javaFeaturegroupsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------+---------+---------+---------+---------+------------------+-------------+\n",
      "|pagerank|triangle_count|  avg_trx|count_trx|  max_trx|  min_trx|  balance|number_of_accounts|          pep|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+------------------+-------------+\n",
      "|     1.0|           5.0| 1090.509|       16|2094.9958| 390.4109|12920.496|                10| 309237645312|\n",
      "|     1.0|           5.0| 738.1404|       16|1464.5397| 9.235389| 11096.28|                 7|1331439861760|\n",
      "|     1.0|           6.0|899.89594|       30|1828.2426|33.797318|1868.0168|                14| 309237645312|\n",
      "|     1.0|           4.0|607.17773|        4|636.18713|578.16833| 9278.589|                 1| 309237645312|\n",
      "|     1.0|           9.0| 698.5791|       28| 1582.427|119.73669| 593.9806|                11|1331439861760|\n",
      "+--------+--------------+---------+---------+---------+---------+---------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a training dataset is created you can specify metadata about the training dataset or set it to null and fill it in later in the feature registry UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDatasetName: String = AML_dataset\n",
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "dataFormat: String = tfrecords\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = Dataset with features for training an AML model\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"AML_dataset\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"tfrecords\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"Dataset with features for training an AML model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.createTrainingDataset(\n",
    "    spark, datasetDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    1, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDatasetName: String = TestDataset\n",
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "dataFormat: String = csv\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = Dataset for Demo purposes\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"TestDataset\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"csv\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"Dataset for Demo purposes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.createTrainingDataset(\n",
    "    spark, datasetDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    1, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Into an Existing Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a dataset have been created, its metadata is browsable in the featurestore registry in the Hopsworks UI. If you don't want to create a new training dataset but just overwrite new data into an existing training dataset (training datasets are immutable and generally stored in binary formats, modifying an existing traning dataset is not supported), you can use the API function `insertIntoTrainingDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDataset: String = TestDataset\n",
      "featurestore: String = fs_demo_featurestore\n",
      "trainingDatasetVersion: Int = 1\n",
      "mode: String = append\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = trx_summary_features without the column count_trx\n"
     ]
    }
   ],
   "source": [
    "val trainingDataset = \"TestDataset\"\n",
    "val featurestore = Hops.getProjectFeaturestore \n",
    "val trainingDatasetVersion = 1 \n",
    "val mode = \"append\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"trx_summary_features without the column count_trx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hops.insertIntoTrainingDataset(\n",
    "    datasetDf, \n",
    "    spark,\n",
    "    trainingDataset,\n",
    "    featurestore,\n",
    "    trainingDatasetVersion,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Dataset Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a **managed** dataset have been created, it is easy to share it and re-use it for training various models. For example if the dataset have been materialized in tf-records format you can call the method `getTrainingDatasetPath(training_dataset)` to get the HDFS path and read it directly in your tensorflow/keras/pytorch code. By default the library will look for the training dataset in the project's featurestore and use version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res34: String = /Projects/fs_demo/Training_Datasets/AML_dataset_1/AML_dataset\n"
     ]
    }
   ],
   "source": [
    "Hops.getTrainingDatasetPath(\"AML_dataset\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Featurestore Metadata\n",
    "\n",
    "To explore the contents of the featurestore we recommend using the featurestore page in the Hopsworks UI but you can also get the metadata programmatically from the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Stores Accessible In the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res35: java.util.List[String] = [fs_demo_featurestore]\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Groups in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res36: java.util.List[String] = [pep_lookup, customer_type_lookup, trx_type_lookup, gender_lookup, industry_sector_lookup, country_lookup, alert_type_lookup, rule_name_lookup, browser_action_lookup, web_address_lookup, demographic_features, alert_features, trx_graph_summary_features, trx_features, trx_summary_features, hipo_features, trx_graph_edge_list, police_report_features, web_logs_features, trx_summary_features_2]\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroups(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Training Datasets in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res37: java.util.List[String] = [AML_dataset, TestDataset]\n"
     ]
    }
   ],
   "source": [
    "Hops.getTrainingDatasets(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Metadata (Features, Feature groups, Training Datasets) for a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res38: io.hops.util.featurestore.FeaturegroupsAndTrainingDatasetsDTO = FeaturegroupsAndTrainingDatasetsDTO{featuregroups=[FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/pep_lookup_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/customer_type_lookup_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/trx_type_lookup_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/gender_lookup_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/industry_sector_lookup_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive..."
     ]
    }
   ],
   "source": [
    "Hops.getFeaturestoreMetadata(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}